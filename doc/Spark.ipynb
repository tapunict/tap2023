{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Unified engine for large-scale data analytics\n",
    "Apache Spark™ is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters \n",
    "![](https://spark.apache.org/images/spark-logo-trademark.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Apache Spark project's History\n",
    "Spark was originally written by the founders of Databricks during their time at UC Berkeley. The Spark project started in 2009, was open sourced in 2010, and in 2013 its code was donated to Apache, becoming Apache Spark. The employees of Databricks have written over 75% of the code in Apache Spark and have contributed more than 10 times more code than any other organization. Apache Spark is a sophisticated distributed computation framework for executing code in parallel across many different machines. While the abstractions and interfaces are simple, managing clusters of computers and ensuring production-level stability is not. Databricks makes big data simple by providing Apache Spark as a hosted solution.\n",
    "\n",
    "A Gentle Introduction to Apache Spark on Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Genesis of Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## From Hadoop 1.0\n",
    "- Big Data and Distributed Computing at Google (2004)\n",
    "- Hadoop at Yahoo! (2006)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://media.makeameme.org/created/guys-its.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://minimalistquotes.com/wp-content/uploads/2022/08/simple-things-should-be-simple-and-complex-things-.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The question then became:\n",
    "there a way to make Hadoop and MR simpler and faster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spark 1.0 and beyond\n",
    "- Spark’s Early Years at AMPLab (2009) \n",
    "- First Paper 10-20x faster then map reduce (2010)\n",
    "- Spark 1.0 Released (2014)\n",
    "- Spark 2.0: Unifying DataFrame and Dataset. Structured Streaming (2016)\n",
    "- Spark 3.0: Hadoop 3.0 support, Support for Pandas, SQL Engine Faster (2020)\n",
    "- Spark 3.4: Spark Connect (2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://cc-media-foxit.fichub.com/image/fox-it-mondofox/0177f439-3c0f-44ae-9803-c25f8bfac0dd/flash-vs-superman-game-2jpg-maxw-824.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Run workloads 100x faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Logistic Regression](https://spark.apache.org/images/logistic-regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Apache Spark achieves\n",
    "- high performance for both batch and streaming data\n",
    "- using a state-of-the-art DAG scheduler\n",
    "- a query optimizer\n",
    "- a physical execution engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why Spark is faster ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1. Hardware improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Today’s commodity servers come cheap, with hundreds of gigabytes of memory, multiple cores, and the underlying Unix-based operating system taking advantage of efficient multithreading and parallel processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://external-preview.redd.it/RVpCIxhliY2p5vKF8I-AoCLIoI48yIEpVPXDduTG6Fc.jpg?auto=webp&s=88a001359893e5533423e9886d4d55cfd2dbdf62)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2. Direct Acyclic Graph (DAG) Scheduler and Query Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Provides an efficient computational graph that can usually be decomposed into tasks that are executed in parallel across workers on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://www.researchgate.net/publication/336769100/figure/fig2/AS:817393752371221@1571893265396/Spark-DAG-for-a-WordCount-application-with-two-stages-each-consisting-of-three-tasks.png)\n",
    "\n",
    "https://www.researchgate.net/publication/336769100_Artificial_neural_networks_based_techniques_for_anomaly_detection_in_Apache_Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ease of Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](http://www.quickmeme.com/img/4d/4d4759d82ce65de86834ff151bc8b419f89f4e2f0d003f10a54b236785e3e6d2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Modularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Write applications quickly in Java, Scala, Python, R, and SQL.\n",
    "\n",
    "Spark offers over 80 high-level operators that make it easy to build parallel apps. \n",
    "\n",
    "And you can use it **interactively** from the Scala, Python, R, and SQL shells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Scala Example\n",
    "\n",
    "\n",
    "```scala\n",
    "df = spark.read.json(\"logs.json\") \n",
    "df.where(\"age > 21\").select(\"name.first\").show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Combine SQL, streaming, and complex analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Spark powers a stack of libraries including "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- SQL and DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- MLlib for machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- GraphX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Spark Streaming. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You can combine these libraries seamlessly in the same application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://spark.apache.org/images/spark-stack.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Runs everywhere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://images2.corriereobjects.it/methode_image/socialshare/2014/10/07/f143a1aa-4e22-11e4-b38c-5070a4632162.jpg)\n",
    "\n",
    "https://www.corriere.it/foto-gallery/esteri/14_ottobre_07/nuovo-attrezzo-fare-sport-ruota-criceti-misura-d-uomo-809cb22a-4e22-11e4-b38c-5070a4632162.shtml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Spark runs on Hadoop, Apache Mesos, Kubernetes, standalone, or in the cloud. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You can run Spark using its standalone cluster mode, on EC2, on Hadoop YARN, on Mesos, or on Kubernetes\n",
    "![](https://spark.apache.org/images/spark-runs-everywhere.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### It can access diverse external data sources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Analyse\n",
    "Spark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat.\n",
    "\n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html#external-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Query\n",
    "Spark SQL supports operating on a variety of data sources through the DataFrame interface. A DataFrame can be operated on using relational transformations and can also be used to create a temporary view. Registering a DataFrame as a temporary view allows you to run SQL queries over its data.\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-data-sources.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Docker\n",
    "Download from https://spark.apache.org/downloads.html into spark/setup\n",
    "\n",
    "We are going to use Spark 3.4.0 Prebuilt for Hadoop 3.3 and later \n",
    "\n",
    "https://dlcdn.apache.org/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dockerfile\n",
    "spark/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Spark Manager\n",
    "spark/spark-manager.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SparkPI\n",
    "https://github.com/apache/spark/blob/master/examples/src/main/python/pi.py\n",
    "\n",
    "Use Monte Carlo Method  https://theabbie.github.io/blog/estimate-pi-using-random-numbers.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Run sparkExamplePi.sh\n",
    "```bash\n",
    "#!/usr/bin/env bash\n",
    "# Stop\n",
    "docker stop sparkPi\n",
    "\n",
    "# Remove previuos container \n",
    "docker container rm sparkPi\n",
    "\n",
    "docker build ../spark/ --tag tap:spark\n",
    "docker run -e SPARK_ACTION=example --network tap --name sparkPi -it tap:spark SparkPi 10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```\n",
    "Running action example\n",
    "Running example ARGS SparkPi 100\n",
    "21/04/18 15:31:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
    "21/04/18 15:31:59 INFO SparkContext: Running Spark version 3.1.1\n",
    "21/04/18 15:31:59 INFO ResourceUtils: ==============================================================\n",
    "21/04/18 15:31:59 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
    "21/04/18 15:31:59 INFO ResourceUtils: ==============================================================\n",
    "21/04/18 15:31:59 INFO SparkContext: Submitted application: Spark Pi\n",
    "\n",
    "...\n",
    "Pi is roughly 3.1419099141909914\n",
    "21/04/18 15:32:09 INFO SparkUI: Stopped Spark web UI at http://958a11429922:4040\n",
    "21/04/18 15:32:09 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
    "21/04/18 15:32:09 INFO MemoryStore: MemoryStore cleared\n",
    "21/04/18 15:32:09 INFO BlockManager: BlockManager stopped\n",
    "21/04/18 15:32:09 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
    "21/04/18 15:32:09 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
    "21/04/18 15:32:09 INFO SparkContext: Successfully stopped SparkContext\n",
    "21/04/18 15:32:09 INFO ShutdownHookManager: Shutdown hook called\n",
    "21/04/18 15:32:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-bc591bec-ca63-4e7b-86f4-191684261e8f\n",
    "21/04/18 15:32:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-4dc0f0c2-7124-4fc7-9122-b4ea4789f57f\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Spark Shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```bash\n",
    "./sparkShell.sh\n",
    "### docker run -e SPARK_ACTION=spark-shell --network tap -it tap:spark)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```\n",
    "Spark context Web UI available at http://17827060ca34:4040\n",
    "Spark context available as 'sc' (master = local[2], app id = local-1586889603227).\n",
    "Spark session available as 'spark'.\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.4.5\n",
    "      /_/\n",
    "\n",
    "Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_212)\n",
    "Type in expressions to have them evaluated.\n",
    "Type :help for more information.\n",
    "\n",
    "scala>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```scala\n",
    "scala> val textFile=spark.read.textFile(\"/opt/tap/spark/dataset/lotr_characters.csv\");\n",
    "scala> textFile.count();\n",
    "res0: Long = 912\n",
    "\n",
    "scala> textFile.first();\n",
    "\n",
    "res3: String = birth,death,gender,hair,height,name,race,realm,spouse\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Resilient Distributed Dataset (RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds\n",
    "\n",
    "Spark revolves around the concept of a resilient distributed dataset (RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://1.bp.blogspot.com/-wMroEy8Ow-k/WdCUxRefTTI/AAAAAAAABNM/Z14px-DgqGYqPfAfwNIILI9EX-ozLGplQCLcBGAs/s640/apache-spark-streaming-13-638.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "fault-tolerant "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://i.imgflip.com/1dzjjc.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "collection of elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://mallikarjuna_g.gitbooks.io/spark/content/diagrams/spark-rdds.png)\n",
    "\n",
    "> https://books.japila.pl/apache-spark-internals/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "that can be operated on in parallel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://mallikarjuna_g.gitbooks.io/spark/content/diagrams/spark-rdd-partitioned-distributed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> Learning Spark \n",
    "\n",
    "An RDD in Spark is simply an immutable distributed collection of objects. \n",
    "\n",
    "Each RDD is split into multiple partitions, which may be computed on different nodes of the cluster. \n",
    "\n",
    "RDDs can contain any type of Python, Java, or Scala objects, including user-defined classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are two ways to create RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "parallelizing an existing collection in your driver program, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "referencing a dataset in an external storage system, \n",
    "such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Py Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Start Py Spark Docker\n",
    "```bash\n",
    "./pyspark.sh \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Library/Java/JavaVirtualMachines/zulu-17.jdk/Contents/Home\n",
      "/Users/nics/Dev/spark-3.4.0-bin-hadoop3\n",
      "/Users/nics/Dev/spark-3.4.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip\n",
      "Python 3.9.15\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo $JAVA_HOME\n",
    "echo $SPARK_HOME\n",
    "echo $PYTHONPATH\n",
    "python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'findspark' from '/Users/nics/miniforge3/lib/python3.9/site-packages/findspark.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pyspark\n",
    "findspark.find() \n",
    "findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/27 18:32:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://151.97.61.237:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Tap</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=Tap>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = pyspark.SparkConf().setAppName('Tap').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List\n",
    "data = [1, 2, 3, 4, 5] \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:287"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distData = sc.parallelize(data)\n",
    "distData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distData.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/Users/nics/Dev/GitHub/tap-workspace/tap2023/spark/dataset/The Return Of The King_djvu.txt MapPartitionsRDD[2] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An RDD can be also created from external storage\n",
    "# textFile creates a RDD(String) (remember when we use spark.read.file)\n",
    "distFile = sc.textFile(\"/Users/nics/Dev/GitHub/tap-workspace/tap2023/spark/dataset/The Return Of The King_djvu.txt\") # Path may be different in your local env\n",
    "distFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"trti '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distFile.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "710716"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizeOfBook=distFile.map(lambda s: len(s)).reduce(lambda a, b: a + b)\n",
    "sizeOfBook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappa=distFile.map(lambda s: len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 0, 0, 0, 24, 0, 11, 0, 11, 12, 0, 0, 0, 14, 0, 0, 0, 0, 11, 0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mappa.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce=mappa.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "710716"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Key Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "While most Spark operations work on RDDs containing any type of objects, a few special operations are only available on RDDs of key-value pairs. The most common ones are distributed “shuffle” operations, such as grouping or aggregating the elements by a key.\n",
    "\n",
    "In Python, these operations work on RDDs containing built-in Python tuples such as (1, 2). Simply create such tuples and then call your desired operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[12] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = distFile.map(lambda s: (s, 1))\n",
    "pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have create a new RDD, let's see what it contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\"trti ', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('“THE LORD OF THE RINGS” ', 1),\n",
       " ('', 1),\n",
       " ('Pjrt Thttt ', 1),\n",
       " ('', 1),\n",
       " ('THE RETURN ', 1),\n",
       " ('OF THE KING ', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('J.R.R.ToIkien ', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('* BOOK V * ', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('Chapter 1 . Minas Tirith ', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('Pippin looked out from the shelter of Gandalf s cloak. He wondered if ', 1),\n",
       " ('he was awake or still sleeping, still in the swift -moving dream in which he ',\n",
       "  1),\n",
       " ('had been wrapped so long since the great ride began. The dark world was ',\n",
       "  1),\n",
       " ('rushing by and the wind sang loudly in his ears. He could see nothing but ',\n",
       "  1),\n",
       " ('the wheeling stars, and away to his right vast shadows against the sky where ',\n",
       "  1),\n",
       " ('the mountains of the South marched past. Sleepily he tried to reckon the ',\n",
       "  1),\n",
       " ('times and stages of their journey, but his memory was drowsy and uncertain. ',\n",
       "  1),\n",
       " ('', 1),\n",
       " ('There had been the first ride at terrible speed without a halt, and ', 1),\n",
       " ('then in the dawn he had seen a pale gleam of gold, and they had come to the ',\n",
       "  1),\n",
       " ('silent town and the great empty house on the hill. And hardly had they ',\n",
       "  1),\n",
       " ('reached its shelter when the winged shadow had passed over once again, and ',\n",
       "  1),\n",
       " ('men wilted with fear. But Gandalf had spoken soft words to him, and he had ',\n",
       "  1),\n",
       " ('slept in a corner, tired but uneasy, dimly aware of comings and goings and ',\n",
       "  1),\n",
       " ('of men talking and Gandalf giving orders. And then again riding, riding in ',\n",
       "  1),\n",
       " ('the night. This was the second, no, the third night since he had looked in ',\n",
       "  1),\n",
       " ('the Stone. And with that hideous memory he woke fully, and shivered, and the ',\n",
       "  1),\n",
       " ('noise of the wind became filled with menacing voices. ', 1),\n",
       " ('', 1),\n",
       " ('A light kindled in the sky, a blaze of yellow fire behind dark barriers ',\n",
       "  1),\n",
       " ('Pippin cowered back, afraid for a moment, wondering into what dreadful ',\n",
       "  1),\n",
       " ('country Gandalf was bearing him. He rubbed his eyes, and then he saw that it ',\n",
       "  1),\n",
       " ('was the moon rising above the eastern shadows, now almost at the full. So ',\n",
       "  1),\n",
       " ('the night was not yet old and for hours the dark journey would go on. He ',\n",
       "  1)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs.take(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we can use a reduce function, to count how may times the line appears in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[19] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = pairs.reduceByKey(lambda a, b: a + b)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\"trti ', 1),\n",
       " ('', 3915),\n",
       " ('“THE LORD OF THE RINGS” ', 1),\n",
       " ('Pjrt Thttt ', 1),\n",
       " ('THE RETURN ', 1),\n",
       " ('OF THE KING ', 1),\n",
       " ('J.R.R.ToIkien ', 1),\n",
       " ('* BOOK V * ', 1),\n",
       " ('Chapter 1 . Minas Tirith ', 1),\n",
       " ('Pippin looked out from the shelter of Gandalf s cloak. He wondered if ', 1),\n",
       " ('he was awake or still sleeping, still in the swift -moving dream in which he ',\n",
       "  1),\n",
       " ('had been wrapped so long since the great ride began. The dark world was ',\n",
       "  1),\n",
       " ('rushing by and the wind sang loudly in his ears. He could see nothing but ',\n",
       "  1),\n",
       " ('the wheeling stars, and away to his right vast shadows against the sky where ',\n",
       "  1),\n",
       " ('the mountains of the South marched past. Sleepily he tried to reckon the ',\n",
       "  1),\n",
       " ('times and stages of their journey, but his memory was drowsy and uncertain. ',\n",
       "  1),\n",
       " ('There had been the first ride at terrible speed without a halt, and ', 1),\n",
       " ('then in the dawn he had seen a pale gleam of gold, and they had come to the ',\n",
       "  1),\n",
       " ('silent town and the great empty house on the hill. And hardly had they ',\n",
       "  1),\n",
       " ('reached its shelter when the winged shadow had passed over once again, and ',\n",
       "  1),\n",
       " ('men wilted with fear. But Gandalf had spoken soft words to him, and he had ',\n",
       "  1),\n",
       " ('slept in a corner, tired but uneasy, dimly aware of comings and goings and ',\n",
       "  1),\n",
       " ('of men talking and Gandalf giving orders. And then again riding, riding in ',\n",
       "  1),\n",
       " ('the night. This was the second, no, the third night since he had looked in ',\n",
       "  1),\n",
       " ('the Stone. And with that hideous memory he woke fully, and shivered, and the ',\n",
       "  1),\n",
       " ('noise of the wind became filled with menacing voices. ', 1),\n",
       " ('A light kindled in the sky, a blaze of yellow fire behind dark barriers ',\n",
       "  1),\n",
       " ('Pippin cowered back, afraid for a moment, wondering into what dreadful ',\n",
       "  1),\n",
       " ('country Gandalf was bearing him. He rubbed his eyes, and then he saw that it ',\n",
       "  1),\n",
       " ('was the moon rising above the eastern shadows, now almost at the full. So ',\n",
       "  1),\n",
       " ('the night was not yet old and for hours the dark journey would go on. He ',\n",
       "  1),\n",
       " ('stirred and spoke. ', 1),\n",
       " (\"’Where are we, Gandalf?' he asked. \", 1),\n",
       " (\"'In the realm of Gondor,' the wizard answered. 'The land of Anurien is \",\n",
       "  1),\n",
       " (\"still passing by.' \", 1),\n",
       " (\"There was a silence again for a while. Then, 'What is that?' cried \", 1),\n",
       " (\"Pippin suddenly, clutching at Gandalf s cloak. 'Look! Fire, red fire! Are \",\n",
       "  1),\n",
       " (\"there dragons in this land? Look, there is another!' \", 1),\n",
       " (\"For answer Gandalf cried aloud to his horse. 'On, Shadowfax! We must \", 1),\n",
       " ('hasten. Time is short. See! The beacons of Gondor are alight, calling for ',\n",
       "  1),\n",
       " ('aid. War is kindled. See, there is the fire on Amon Don, and flame on ', 1),\n",
       " ('Eilenach; and there they go speeding west: Nardol, Erelas, Min-Rimmon, ',\n",
       "  1),\n",
       " (\"Calenhad, and the Elalifirien on the borders of Rohan.' \", 1),\n",
       " ('But Shadowfax paused in his stride, slowing to a walk, and then he ', 1),\n",
       " ('lifted up his head and neighed. And out of the darkness the answering neigh ',\n",
       "  1),\n",
       " ('of other horses came; and presently the thudding of hoofs was heard, and ',\n",
       "  1),\n",
       " ('three riders swept up and passed like flying ghosts in the moon and vanished ',\n",
       "  1),\n",
       " ('into the West. Then Shadowfax gathered himself together and sprang away, ',\n",
       "  1),\n",
       " ('and ', 8),\n",
       " ('the night flowed over him like a roaring wind. ', 1)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.take(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's order by key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered=counts.sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 3915),\n",
       " ('\"The Lords of Gondor are come! Let all leave this land or yield them up!\\' ',\n",
       "  1),\n",
       " ('\"Thus spoke Malbeth the Seer, in the days of Arvedui, last king at ', 1),\n",
       " ('\"gatherers\" and \"sharers\", I reckon, going round counting and measuring and ',\n",
       "  1),\n",
       " ('\"trti ', 1),\n",
       " ('\\' \"At Pelargir the Heir of Isildur will have need of you,\" he said. ', 1),\n",
       " ('\\' \"Hear now the words of the Heir of Isildur! Your oath is fulfilled. ',\n",
       "  1),\n",
       " ('\\' \"I\\'ll give you Sharkey, you dirty thieving ruffians!\" says she, and ',\n",
       "  1),\n",
       " ('\\' \"It is forty leagues and two from Pelargir to the landings at the ', 1),\n",
       " ('\\' \"Sharkey,\" says they. \"So get out o\\' the road, old hagling!\" ', 1)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered.takeOrdered(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's do a better analysis\n",
    "Which is the most frequent word in the book ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "words=distFile.flatMap(lambda line:line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"trti',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '“THE',\n",
       " 'LORD',\n",
       " 'OF',\n",
       " 'THE',\n",
       " 'RINGS”',\n",
       " '',\n",
       " '',\n",
       " 'Pjrt',\n",
       " 'Thttt',\n",
       " '',\n",
       " '',\n",
       " 'THE',\n",
       " 'RETURN',\n",
       " '',\n",
       " 'OF',\n",
       " 'THE',\n",
       " 'KING',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'J.R.R.ToIkien',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '*',\n",
       " 'BOOK',\n",
       " 'V',\n",
       " '*',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Chapter',\n",
       " '1',\n",
       " '.',\n",
       " 'Minas',\n",
       " 'Tirith',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Pippin',\n",
       " 'looked',\n",
       " 'out',\n",
       " 'from',\n",
       " 'the',\n",
       " 'shelter',\n",
       " 'of',\n",
       " 'Gandalf',\n",
       " 's',\n",
       " 'cloak.',\n",
       " 'He',\n",
       " 'wondered',\n",
       " 'if',\n",
       " '',\n",
       " 'he',\n",
       " 'was',\n",
       " 'awake',\n",
       " 'or',\n",
       " 'still',\n",
       " 'sleeping,',\n",
       " 'still',\n",
       " 'in',\n",
       " 'the',\n",
       " 'swift',\n",
       " '-moving',\n",
       " 'dream',\n",
       " 'in',\n",
       " 'which',\n",
       " 'he',\n",
       " '',\n",
       " 'had',\n",
       " 'been',\n",
       " 'wrapped',\n",
       " 'so',\n",
       " 'long',\n",
       " 'since',\n",
       " 'the',\n",
       " 'great',\n",
       " 'ride',\n",
       " 'began.',\n",
       " 'The',\n",
       " 'dark',\n",
       " 'world',\n",
       " 'was',\n",
       " '',\n",
       " 'rushing',\n",
       " 'by',\n",
       " 'and',\n",
       " 'the',\n",
       " 'wind',\n",
       " 'sang']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Great, let's assign a counter and then sum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "wordCounters=words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\"trti', 1),\n",
       " ('', 14916),\n",
       " ('“THE', 1),\n",
       " ('LORD', 2),\n",
       " ('OF', 5),\n",
       " ('THE', 8),\n",
       " ('RINGS”', 1),\n",
       " ('Pjrt', 1),\n",
       " ('Thttt', 1),\n",
       " ('RETURN', 2)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCounters.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ok I want to sort now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 14916),\n",
       " ('the', 8035),\n",
       " ('and', 5811),\n",
       " ('of', 3964),\n",
       " ('to', 2768),\n",
       " ('a', 2337),\n",
       " ('in', 2004),\n",
       " ('he', 1923),\n",
       " ('that', 1644),\n",
       " ('was', 1505),\n",
       " ('his', 1301),\n",
       " ('I', 1269),\n",
       " ('it', 1081),\n",
       " ('they', 1041),\n",
       " ('you', 990),\n",
       " ('for', 903),\n",
       " ('as', 898),\n",
       " ('not', 890),\n",
       " ('with', 868),\n",
       " ('said', 847),\n",
       " ('had', 810),\n",
       " ('is', 805),\n",
       " ('at', 759),\n",
       " ('all', 696),\n",
       " ('on', 688),\n",
       " ('have', 648),\n",
       " ('be', 646),\n",
       " ('but', 635),\n",
       " ('were', 617),\n",
       " ('from', 594),\n",
       " ('And', 552),\n",
       " ('But', 531),\n",
       " ('will', 516),\n",
       " ('their', 484),\n",
       " ('there', 482),\n",
       " ('The', 469),\n",
       " ('now', 451),\n",
       " ('no', 423),\n",
       " ('came', 422),\n",
       " ('if', 408),\n",
       " ('or', 404),\n",
       " ('great', 401),\n",
       " ('we', 396),\n",
       " ('He', 394),\n",
       " ('my', 367),\n",
       " ('are', 362),\n",
       " ('by', 357),\n",
       " ('him', 353),\n",
       " ('out', 351),\n",
       " ('up', 347),\n",
       " ('would', 319),\n",
       " ('your', 317),\n",
       " ('them', 307),\n",
       " ('could', 305),\n",
       " ('this', 298),\n",
       " ('into', 291),\n",
       " ('like', 287),\n",
       " ('upon', 275),\n",
       " ('then', 266),\n",
       " ('when', 260),\n",
       " ('one', 258),\n",
       " ('so', 257),\n",
       " ('been', 256),\n",
       " ('long', 256),\n",
       " ('more', 254),\n",
       " ('some', 253),\n",
       " ('come', 249),\n",
       " ('Sam', 248),\n",
       " ('went', 247),\n",
       " ('than', 243),\n",
       " ('before', 240),\n",
       " ('down', 240),\n",
       " ('what', 237),\n",
       " (\"'I\", 236),\n",
       " ('Then', 233),\n",
       " ('do', 232),\n",
       " ('shall', 222),\n",
       " ('last', 221),\n",
       " ('still', 214),\n",
       " ('It', 207),\n",
       " ('its', 205),\n",
       " ('about', 205),\n",
       " ('men', 197),\n",
       " ('many', 196),\n",
       " ('go', 195),\n",
       " ('has', 194),\n",
       " ('any', 192),\n",
       " ('back', 192),\n",
       " ('Frodo', 190),\n",
       " ('me', 190),\n",
       " ('looked', 188),\n",
       " ('must', 185),\n",
       " ('only', 181),\n",
       " ('far', 178),\n",
       " ('They', 177),\n",
       " ('see', 172),\n",
       " ('yet', 172),\n",
       " ('did', 171),\n",
       " ('said.', 170),\n",
       " ('For', 169),\n",
       " ('saw', 167),\n",
       " ('stood', 166),\n",
       " ('who', 164),\n",
       " ('over', 163),\n",
       " ('seemed', 160),\n",
       " ('she', 159),\n",
       " ('an', 156),\n",
       " ('even', 156),\n",
       " ('Gandalf', 155),\n",
       " ('where', 155),\n",
       " ('Pippin', 154),\n",
       " ('may', 153),\n",
       " ('our', 150),\n",
       " ('away', 148),\n",
       " ('through', 140),\n",
       " ('passed', 139),\n",
       " ('should', 139),\n",
       " ('old', 137),\n",
       " ('can', 137),\n",
       " ('Aragorn', 136),\n",
       " ('her', 135),\n",
       " ('little', 133),\n",
       " ('dark', 130),\n",
       " ('There', 129),\n",
       " ('other', 129),\n",
       " (\"'But\", 126),\n",
       " ('Merry', 125),\n",
       " ('him,', 123),\n",
       " ('set', 122),\n",
       " ('while', 122),\n",
       " ('turned', 121),\n",
       " ('it,', 121),\n",
       " ('time', 120),\n",
       " ('am', 119),\n",
       " ('him.', 118),\n",
       " ('though', 118),\n",
       " ('Lord', 116),\n",
       " ('heard', 116),\n",
       " ('which', 115),\n",
       " ('very', 115),\n",
       " ('under', 114),\n",
       " ('day', 112),\n",
       " ('At', 112),\n",
       " ('rode', 112),\n",
       " ('made', 111),\n",
       " ('Mr.', 111),\n",
       " ('know', 109),\n",
       " ('way', 109),\n",
       " ('black', 108),\n",
       " ('hope', 106),\n",
       " ('road', 106),\n",
       " ('those', 105),\n",
       " ('again', 104),\n",
       " ('such', 104),\n",
       " ('much', 104),\n",
       " ('light', 103),\n",
       " ('behind', 103),\n",
       " ('ever', 103),\n",
       " ('after', 101),\n",
       " ('left', 101),\n",
       " ('until', 101),\n",
       " ('it.', 100),\n",
       " ('You', 100),\n",
       " ('eyes', 100),\n",
       " ('A', 99),\n",
       " ('days', 99),\n",
       " ('them.', 98),\n",
       " ('things', 98),\n",
       " ('going', 97),\n",
       " ('us', 96),\n",
       " ('thought', 95),\n",
       " (\"'And\", 94),\n",
       " ('lay', 94),\n",
       " ('own', 93),\n",
       " ('first', 92),\n",
       " ('beyond', 92),\n",
       " ('soon', 92),\n",
       " ('too', 91),\n",
       " ('hand', 90),\n",
       " ('think', 90),\n",
       " ('fell', 89),\n",
       " ('might', 88),\n",
       " ('towards', 88),\n",
       " ('here', 87),\n",
       " ('you,', 87),\n",
       " ('In', 87),\n",
       " ('high', 86),\n",
       " ('took', 86),\n",
       " ('Gondor', 84),\n",
       " ('these', 84),\n",
       " ('City', 83),\n",
       " ('grey', 83),\n",
       " ('take', 83),\n",
       " ('them,', 83),\n",
       " ('above', 82),\n",
       " ('Faramir', 81),\n",
       " ('off', 81),\n",
       " ('two', 80),\n",
       " ('white', 79),\n",
       " ('fear', 79)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsSorted=wordCounters.takeOrdered(200, key = lambda x: -x[1])\n",
    "wordsSorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biblio\n",
    "- https://www.kdnuggets.com/2017/08/three-apache-spark-apis-rdds-dataframes-datasets.html\n",
    "- https://www.slideshare.net/differentsachin/apache-spark-introduction-and-resilient-distributed-dataset-basics-and-deep-dive\n",
    "- https://data-flair.training/blogs/apache-spark-rdd-vs-dataframe-vs-dataset/\n",
    "- https://www.slideshare.net/taposhdr/resilient-distributed-datasets\n",
    "- http://vishnuviswanath.com/spark_rdd.html\n",
    "- https://sparkbyexamples.com/apache-spark-rdd/spark-rdd-actions/\n",
    "- https://www.educba.com/rdd-in-spark/\n",
    "- https://www.javahelps.com/2019/02/spark-03-understanding-resilient.html\n",
    "- https://www.tutorialspoint.com/apache_spark/apache_spark_rdd.htm\n",
    "- https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf\n",
    "- https://www.sicara.ai/blog/2017-05-02-get-started-pyspark-jupyter-notebook-3-minutes"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": "true",
   "footer": "<div class=\"tap-footer\"> *** Technologies for advanced programming (TAP) - 2022 ***</div>",
   "header": "<div class=\"tap-header\"></div>",
   "scroll": true,
   "theme": "white"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
