{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8371ee4b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark (h)and(s)-on Data Bricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ddaec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cd31ec5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Bricks\n",
    "https://www.databricks.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcc82fe",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Data Lakehouse Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a6aa9a",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Simple. Open. Multicloud.**\n",
    "\n",
    "The Databricks Lakehouse Platform combines the best elements of data lakes and data warehouses to deliver the reliability, strong governance and performance of data warehouses with the openness, flexibility and machine learning support of data lakes.\n",
    "\n",
    "This unified approach simplifies your modern data stack by eliminating the data silos that traditionally separate and complicate data engineering, analytics, BI, data science and machine learning. It’s built on open source and open standards to maximize flexibility. And, its common approach to data management, security and governance helps you operate more efficiently and innovate faster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680730c8",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://www.databricks.com/en-website-assets/static/5a945e175f09eb2f522b911352a149f2/Marketecture.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc83920c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f797ec2d",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The unified approach simplifies your data architecture by eliminating the data silos that traditionally separate analytics, BI, data science and machine learning. With a lakehouse, you can eliminate the complexity and expense that make it hard to achieve the full potential of your analytics and AI initiatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf2d8b3",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/databricks-simple.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25acac16",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Open"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd384141",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Delta Lake forms the open foundation of the lakehouse by providing reliability and world-record-setting performance directly on data in the data lake. You’re able to avoid proprietary walled gardens, easily share data, and build your modern data stack with unrestricted access to the ecosystem of open source data projects and the broad Databricks partner network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496a55d1",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://www.databricks.com/en-website-assets/static/e3d1995c123d1a62316de195faff275e/3024c/Platform-orignal-creators.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9da8a6c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Multicloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61cac18",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The Databricks Lakehouse Platform offers you a consistent management, security, and governance experience across all clouds. You don’t need to invest in reinventing processes for every cloud platform that you’re using to support your data and AI efforts. Instead, your data teams can simply focus on putting all your data work to discover new insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c9d827",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "![](https://www.databricks.com/en-website-assets/static/f4d140f58197009a3d0953ce3651ccc8/Multicloud.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce9966f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Community Edition \n",
    "https://community.cloud.databricks.com/login.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172fb490",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Create Account\n",
    "- Sign Up for a new account\n",
    "- Select \"Get started with Community Edition\" instead of selecting a Cloud Provider\n",
    "- Verify the email"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bd566e",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Run Tutorial\n",
    "- Click on Guide: Quickstart tutorial\n",
    "- Create a Cluster clicking on Connect\n",
    "- Run Cells in order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3886e4fd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2c3231",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Unified engine for large-scale data analytics\n",
    "Apache Spark™ is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters \n",
    "![](https://spark.apache.org/images/spark-logo-trademark.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35e0b61",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Apache Spark project's History\n",
    "Spark was originally written by the founders of Databricks during their time at UC Berkeley. The Spark project started in 2009, was open sourced in 2010, and in 2013 its code was donated to Apache, becoming Apache Spark. The employees of Databricks have written over 75% of the code in Apache Spark and have contributed more than 10 times more code than any other organization. Apache Spark is a sophisticated distributed computation framework for executing code in parallel across many different machines. While the abstractions and interfaces are simple, managing clusters of computers and ensuring production-level stability is not. Databricks makes big data simple by providing Apache Spark as a hosted solution.\n",
    "\n",
    "A Gentle Introduction to Apache Spark on Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d704ba",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Genesis of Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a11d464",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## From Hadoop 1.0\n",
    "- Big Data and Distributed Computing at Google (2004)\n",
    "- Hadoop at Yahoo! (2006)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ddbac8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://media.makeameme.org/created/guys-its.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b94bbc",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://minimalistquotes.com/wp-content/uploads/2022/08/simple-things-should-be-simple-and-complex-things-.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327cf737",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The question then became:\n",
    "there a way to make Hadoop and MR simpler and faster?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a28c8c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spark 1.0 and beyond\n",
    "- Spark’s Early Years at AMPLab (2009) \n",
    "- First Paper 10-20x faster then map reduce (2010)\n",
    "- Spark 1.0 Released (2014)\n",
    "- Spark 2.0: Unifying DataFrame and Dataset. Structured Streaming (2016)\n",
    "- Spark 3.0: Hadoop 3.0 support, Support for Pandas, SQL Engine Faster (2020)\n",
    "- Spark 3.4: Spark Connect (2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6bd947",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b09d529",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1. Speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e42083",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://cc-media-foxit.fichub.com/image/fox-it-mondofox/0177f439-3c0f-44ae-9803-c25f8bfac0dd/flash-vs-superman-game-2jpg-maxw-824.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed4e2e5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Run workloads 100x faster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83614c27",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Logistic Regression](https://spark.apache.org/images/logistic-regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943bf037",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Apache Spark achieves\n",
    "- high performance for both batch and streaming data\n",
    "- using a state-of-the-art DAG scheduler\n",
    "- a query optimizer\n",
    "- a physical execution engine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a80d446",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why Spark is faster ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2aa939a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1. Hardware improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ce78d1",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Today’s commodity servers come cheap, with hundreds of gigabytes of memory, multiple cores, and the underlying Unix-based operating system taking advantage of efficient multithreading and parallel processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae48cad8",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://external-preview.redd.it/RVpCIxhliY2p5vKF8I-AoCLIoI48yIEpVPXDduTG6Fc.jpg?auto=webp&s=88a001359893e5533423e9886d4d55cfd2dbdf62)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be7a440",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2. Direct Acyclic Graph (DAG) Scheduler and Query Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8457a90b",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Provides an efficient computational graph that can usually be decomposed into tasks that are executed in parallel across workers on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4770eb4d",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://www.researchgate.net/publication/336769100/figure/fig2/AS:817393752371221@1571893265396/Spark-DAG-for-a-WordCount-application-with-two-stages-each-consisting-of-three-tasks.png)\n",
    "\n",
    "https://www.researchgate.net/publication/336769100_Artificial_neural_networks_based_techniques_for_anomaly_detection_in_Apache_Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7c7007",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3. Physical execution engine "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d402fb7",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Tungsten is the codename for the umbrella project to make changes to Apache Spark’s execution engine that focuses on substantially improving the efficiency of memory and CPU for Spark applications, to push performance closer to the limits of modern hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b924baf3",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://shuttermuse.com/wp-content/uploads/2015/03/what-is-a-tungsten-light.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d531b2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2. Ease of Use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120366f8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Modularity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b311a872",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Write applications quickly in Java, Scala, Python, R, and SQL.\n",
    "\n",
    "Spark offers over 80 high-level operators that make it easy to build parallel apps. \n",
    "\n",
    "And you can use it **interactively** from the Scala, Python, R, and SQL shells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbb70bf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Scala Example\n",
    "\n",
    "\n",
    "```scala\n",
    "df = spark.read.json(\"logs.json\") \n",
    "df.where(\"age > 21\").select(\"name.first\").show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6bab5e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e533ae6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Combine SQL, streaming, and complex analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f26c59a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Spark powers a stack of libraries including "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302ad671",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- SQL and DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abf99ec",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- MLlib for machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e08329",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- GraphX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb2766c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff54177a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://spark.apache.org/images/spark-stack.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2948f146",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### It can access diverse external data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e607ca1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Analyse\n",
    "Spark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat.\n",
    "\n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html#external-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb7f35e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Query\n",
    "Spark SQL supports operating on a variety of data sources through the DataFrame interface. A DataFrame can be operated on using relational transformations and can also be used to create a temporary view. Registering a DataFrame as a temporary view allows you to run SQL queries over its data.\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-data-sources.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2f6b61",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Examples\n",
    "https://spark.apache.org/examples.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fb2695",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Compute Spark PI\n",
    "https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/1408031979081866/1514004441740508/2956912205716139/latest.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301d6132",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Word Count\n",
    "https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/1408031979081866/1177351990298623/2956912205716139/latest.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2ed226",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data Frame Text Search\n",
    "https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/1408031979081866/704214092842249/2956912205716139/latest.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dcd573",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6777335",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Spark has several core abstractions: Datasets, DataFrames, SQL Tables, and Resilient Distributed Datasets (RDDs).\n",
    "\n",
    "These abstractions all represent distributed collections of data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef6062b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Transformation\n",
    "\n",
    "In Spark, the core data structures are immutable meaning they cannot be changed once created. This might seem like a strange concept at first, if you cannot change it, how are you supposed to use it? In order to “change” a DataFrame you will have to instruct Spark how you would like to modify the DataFrame you have into the one that you want. These instructions are called transformations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5964178c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Resilient Distribuited Datasets (RDD)\n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3ff74f",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Spark revolves around the concept of a resilient distributed dataset (RDD), which is a fault-tolerant collection of elements that can be operated on in parallel. There are two ways to create RDDs: parallelizing an existing collection in your driver program, or referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de9d767",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://1.bp.blogspot.com/-wMroEy8Ow-k/WdCUxRefTTI/AAAAAAAABNM/Z14px-DgqGYqPfAfwNIILI9EX-ozLGplQCLcBGAs/s640/apache-spark-streaming-13-638.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebe1e2d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An RDD in Spark is simply an immutable distributed collection of objects. \n",
    "\n",
    "Each RDD is split into multiple partitions, which may be computed on different nodes of the cluster. \n",
    "\n",
    "RDDs can contain any type of Python, Java, or Scala objects, including user-defined classes.\n",
    "\n",
    "> Learning Spark: https://pages.databricks.com/rs/094-YMS-629/images/LearningSpark2.0.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b116a63e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A live example\n",
    "https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/1408031979081866/1455717965234675/2956912205716139/latest.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365b6962",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data Frames\n",
    "https://spark.apache.org/docs/latest/sql-getting-started.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7521ea73",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is a DataFrame?\n",
    "https://www.databricks.com/glossary/what-are-dataframes\n",
    "\n",
    "A DataFrame is a data structure that organizes data into a 2-dimensional table of rows and columns, much like a spreadsheet. DataFrames are one of the most common data structures used in modern data analytics because they are a flexible and intuitive way of storing and working with data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7469938",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Every DataFrame contains a blueprint, known as a schema, that defines the name and data type of each column. Spark DataFrames can contain universal data types like StringType and IntegerType, as well as data types that are specific to Spark, such as StructType. Missing or incomplete values are stored as null values in the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5758cb",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "![](https://www.databricks.com/wp-content/uploads/2018/05/DataFrames.png)\n",
    "\n",
    "Dataframe representation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0fef0e",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A simple analogy is that a DataFrame is like a spreadsheet with named columns. However, the difference between them is that while a spreadsheet sits on one computer in one specific location, a DataFrame can span thousands of computers. In this way, DataFrames make it possible to do analytics on big data, using distributed computing clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37370f08",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7a/Visicalc.png/440px-Visicalc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3819605",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The reason for putting the data on more than one computer should be intuitive: either the data is too large to fit on one machine or it would simply take too long to perform that computation on one machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf39991",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://intellipaat.com/mediaFiles/2015/08/Resilient-Distributed-Datasets-RDDs.jpg)\n",
    "\n",
    "https://intellipaat.com/blog/tutorial/spark-tutorial/programming-with-rdds/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc05961d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DataFrames\n",
    "\n",
    "The concept of a DataFrame is common across many different languages and frameworks. DataFrames are the main data type used in pandas, the popular Python data analysis library, and DataFrames are also used in R, Scala, and other languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e313834",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "![](https://image.slidesharecdn.com/jumpstartintoapachesparkanddatabricks-160212150759/95/jump-start-into-apache-spark-and-databricks-13-638.jpg?cb=1463623478)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e7e502",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://databricks.com/wp-content/uploads/2016/06/Unified-Apache-Spark-2.0-API-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f15b92",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Data Frame Example\n",
    "https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/1408031979081866/3119543398385477/2956912205716139/latest.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af901f36",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "- https://docs.databricks.com/introduction/index.html\n",
    "- https://spark.apache.org/docs/0.8.0/api/pyspark/pyspark.rdd.RDD-class.html\n",
    "- https://docs.databricks.com/dbfs/databricks-datasets.html"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": "true",
   "footer": "<div class=\"tswd-footer\"> *** Cloud Computing and Big Data - 2023 ***</div>",
   "header": "<div class=\"tswd-header\"></div>",
   "scroll": true,
   "theme": "night"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
